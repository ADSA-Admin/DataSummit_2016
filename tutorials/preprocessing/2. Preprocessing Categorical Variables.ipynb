{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Categorical Variables\n",
    "\n",
    "In this section, we'll mention some of the difficulties modeling categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCI Mushroom dataset (mushroom.data)\n",
    "\n",
    "* 22 categorical features and one response variable (label)\n",
    "* Objective is to classify whether a mushroom is edible (\"e\") or poisonous (\"p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "prefix = \"../datasets/\"\n",
    "df = pd.read_csv(prefix + \"mushroom.data\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = [\"edible\", \"cap-shape\", \"cap-surface\", \"cap-color\", \"bruises?\",\n",
    "        \"odor\", \"gill-attachment\", \"gill-spacing\", \"gill-size\", \"gill-color\",\n",
    "        \"stalk-shape\", \"stalk-root\", \"stalk-surface-above-ring\",\n",
    "        \"stalk-surface-below-ring\", \"stalk-color-above-ring\",\n",
    "        \"stalk-color-below-ring\", \"veil-type\", \"veil-color\", \"ring-number\",\n",
    "        \"ring-type\", \"spore-print-color\", \"population\", \"habitat\"\n",
    "        ]\n",
    "\n",
    "df = pd.read_csv(prefix + \"mushroom.data\", sep=\",\", names=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem with categorical variables\n",
    "\n",
    "* We can't plug categorical variables straight away into a classifier; we need to make them numeric feature vectors first\n",
    "\n",
    "    **NOTE**. Machine learning algorithms often involve performing dot products with the input vector. For example,       SVMs will classify an object based upon the following criterion. \n",
    "    \n",
    "    If $\\vec{w}$ is the weight vector and $b$ the bias of an\n",
    "    SVM, and $\\vec{x}$ is the input feature vector, then the label given to $\\vec{x}$, $f(\\vec{x})$, is\n",
    "\n",
    "    $$f(\\vec{x}) = \\text{sign}(\\vec{w}\\cdot \\vec{x} + b) \\quad (1)$$\n",
    "\n",
    "    where \n",
    "    \n",
    "    $$\\text{sign}(x) = \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "        -1 &  \\text{ if } x < 0 \\\\\n",
    "        1  &  \\text{ otherwise}\n",
    "        \\end{array}\\right.$$\n",
    "    \n",
    "    Thus, $\\vec{x}\\in \\mathbb{R}^n$ has to be a vector of $n$ real value numbers. We need an encoding technique.\n",
    "    \n",
    "* One common way to do it: **one hot encodings** (ie. \"dummy encodings\")\n",
    "    * Scikit-Learn implementation: [sklearn.preprocessing.OneHotEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)\n",
    "    * Pandas implementation: [pandas.get_dummies](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
    "        * **Note**: Possible bug in releases below 0.17 (I think)\n",
    "        * Does not necessarily preserve encodings for a training / test set split; training and test sets might have different categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example:\n",
    "\n",
    "features = [[\"red\"],\n",
    "            [\"blue\"],\n",
    "            [\"green\"],\n",
    "            [\"red\"]]\n",
    "\n",
    "sample = pd.DataFrame(data=features, columns=[\"color\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "onehot = pd.get_dummies(sample)\n",
    "onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose that we trained an SVM on a dataset with a single feature (\"color\") with three possible values, \"blue\", \"green\", \"red\". You perform a one hot encoding on your training set, and train your SVM. \n",
    "\n",
    "this 3 sample dataset, and the SVM came up with the following weights and bias after training.\n",
    "\n",
    "$$\\vec{w} = \\begin{pmatrix} 3 \\\\ -2 \\\\ 3 \\\\ \\end{pmatrix} \\qquad b = -1$$\n",
    "\n",
    "($\\vec{w}$ is length 3, because there are 3 categories for this one feature dataset.)\n",
    "\n",
    "Your one hot encoding above would yield\n",
    "\n",
    "$$\n",
    "    \\begin{pmatrix} \n",
    "        \\text{\"red\"} \\\\ \n",
    "        \\text{\"blue\"} \\\\ \n",
    "        \\text{\"green\"} \\\\ \n",
    "        \\text{\"red\"} \n",
    "    \\end{pmatrix} \n",
    "    \\implies \n",
    "    \\begin{pmatrix} \n",
    "    \\text{\"blue\"} & \\text{\"green\"} & \\text{\"red\"} \\\\ \n",
    "        0 & 0 & 1 \\\\ \n",
    "        1 & 0 & 0 \\\\ \n",
    "        0 & 1 & 0 \\\\ \n",
    "        0 & 0 & 1 \n",
    "    \\end{pmatrix}$$\n",
    "\n",
    "and so for each row, you would apply equation (1) from above. For example, for the first sample, you will have\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(\\vec{x}) \n",
    "&= \\text{sign}(\\vec{w}\\cdot \\vec{x}) + b & \\text{by equation (1)}\\\\ \n",
    "&= \\text{sign}\\Bigg( \n",
    "    \\begin{pmatrix} \n",
    "    3 \\\\ -2 \\\\ 3 \n",
    "    \\end{pmatrix} \\cdot \n",
    "    \\begin{pmatrix} \n",
    "    0 \\\\ 0 \\\\ 1 \n",
    "    \\end{pmatrix} \\Bigg) - 1 \\\\\n",
    "&= 1.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "If you're fluent in matrix operations, you'll notice that you can perform equation (1) in one sweep. If $X$ is the matrix of all your samples - each sample as a row in $X$ - the following will compute all labels for each sample in $X$.\n",
    "\n",
    "$$f(X) = X \\cdot w + \\begin{pmatrix} b \\\\ \\vdots \\\\ b \\end{pmatrix} $$\n",
    "\n",
    "where $f(X) \\in \\{0, 1\\}^4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes about one hot encodings\n",
    "\n",
    "* Assumes that each possible category is independent of each other\n",
    "* If a variable has $k$ unique values, the encoding will add $k - 1$ features to your feature vector\n",
    "* No effort to understand semantic meaning of categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df.drop(\"edible\", axis=1)\n",
    "y = df[\"edible\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying a one hot encoding for classification\n",
    "\n",
    "For this, we'll simplify steps and encode the training and test data at the same time. But remember you wouldn't be able to do this in a production environment. (Why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_encoded = pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y)\n",
    "\n",
    "predictions = RandomForestClassifier().fit(X_train, y_train).predict(X_test)\n",
    "print \"Accuracy of random forest: \", accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last words about one hot encodings\n",
    "\n",
    "* Certainly not the best way to encode categorical variables, but it's easy and widely used\n",
    "    * Makes feature vectors very sparse\n",
    "    * Can't enforce dependence between any two categories\n",
    "    * If a new category appears in your testing set which was not present in the training set, you're screwed!\n",
    "        * The way we encoded the dataset above was \"not kosher\". Whoops.\n",
    "* For some models, R will do the encodings for you under the hood\n",
    "    * \"Categorical\" variables in R are called [factor variables](http://www.ats.ucla.edu/stat/r/modules/factor_variables.htm). \n",
    "    * Models in R will detect whether a variable is a factor type before doing anything"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
