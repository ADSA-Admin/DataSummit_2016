{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Separators\n",
    "\n",
    "We'll talk about the following models in this notebook:\n",
    "\n",
    "* Perceptrons\n",
    "* Support vector machines\n",
    "\n",
    "You'll find that each one is just a variation of the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **linear separator** $f : X \\to \\{-1, 1\\}$ is a function that\n",
    "classifies points based upon the following criterion:\n",
    "$$f(\\vec{x}) = \\text{sign}(\\vec{w} \\cdot \\vec{x} + b) \\qquad (1)$$\n",
    "where\n",
    "$$\n",
    "\\text{sign}(a) = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "1 & \\text{ if } a \\geq 0 \\\\\n",
    "-1 & \\text{ otherwise}\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/linear.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More notation: Canonical representations\n",
    "\n",
    "For the rest of this presentation, I'd like to use the **conanical** representation of an input sample $\\vec{x}$:\n",
    "\n",
    "$$\\vec{x}' = \\begin{bmatrix} \\vec{x} \\\\ 1 \\end{bmatrix}$$\n",
    "\n",
    "And with that, I'll also create $\\vec{w}'$:\n",
    "\n",
    "$$\\vec{w}' = \\begin{bmatrix} \\vec{w} \\\\ b \\end{bmatrix}$$\n",
    "\n",
    "With this representation, $(1)$ becomes\n",
    "\n",
    "$$f(\\vec{x}') = \\text{sign}(\\vec{w}' \\cdot \\vec{x}') \\qquad (2)$$\n",
    "\n",
    "which cleans up some of the notation. I'll also let $\\vec{w} \\leftarrow \\vec{w}'$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron (Rosenblatt, 1957) is probably one of the simpliest machine learning models you'll find. And implementations follow the gradient descent framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**: Minimize the following cost per sample:\n",
    "\n",
    "$$c(\\vec{x_i}, y_i) = \\max(0, -y_i (\\vec{w} \\cdot \\vec{x_i})) \\qquad(3)$$\n",
    "\n",
    "which is equivalent to\n",
    "\n",
    "$$c(\\vec{x_i}, y_i) = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "-y_i (\\vec{w} \\cdot \\vec{x_i}) & \\text{if } y_i (\\vec{w} \\cdot \\vec{x}) \\leq 0 \\qquad\\text{misclassification} \\\\\n",
    "                             0 & \\text{otherwise}\n",
    "\\end{array}\n",
    "\\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use stochastic gradient descent to update our weight vector $\\vec{w}$. Refresh on the stochastic gradient descent algorithm...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/sgd-algo2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that calculating $\\nabla_{\\vec{w}} c(\\vec{x}, y)$ is extremely easy!\n",
    "$$\\nabla_{\\vec{w}} = \\Big(\\frac{\\partial f}{\\partial w_1}, \\dots, \\frac{\\partial f}{\\partial w_d}, \\frac{\\partial f}{\\partial b} \\Big)$$\n",
    "\n",
    "<!--\n",
    "Therefore, if we focus on just one of these $\\frac{\\partial f}{\\partial w_i}$, we have\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial w_i} \\Big(c(y, f(\\vec{x})) \\Big) \\Big\\vert_{\\vec{x} = \\vec{x_i}, y = y_i} = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "-y_i \\vec{x}_i & \\text{if } y_i (\\vec{w} \\cdot \\vec{x}) \\leq 0 \\qquad\\text{misclassification}  \\\\\n",
    "0 & \\text{otherwise } \\\\\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore,\n",
    "$$\\nabla_w c(y, f(\\vec{x})) \\Big\\vert_{\\vec{x} = \\vec{x_i}, y_i = y} = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "-y_i \\vec{x_i} & \\text{if } y_i (\\vec{w} \\cdot \\vec{x}) \\leq 0 \\qquad\\text{misclassification} \\\\\n",
    "                             0 & \\text{otherwise}\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which yields the update rule\n",
    "\n",
    "$$\\vec{w}^{(t + 1)} \\leftarrow \\vec{w}^{(t)} + \\eta^{(t)} \n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "y_i \\vec{x}_i & \\text{if } y_i (\\vec{w} \\cdot \\vec{x}) \\leq 0 \\qquad\\text{misclassification} \\\\\n",
    "                             0 & \\text{otherwise}\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/perceptron.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Source: http://stats.stackexchange.com/questions/71335/decision-boundary-plot-for-a-perceptron\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Perceptron\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=200, n_features=2, n_redundant=0, random_state=0)\n",
    "clf = Perceptron(n_iter=100).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = .02  # step size in the mesh\n",
    "\n",
    "# create a mesh to plot in\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "fig, ax = plt.subplots()\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "ax.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "ax.axis('off')\n",
    "\n",
    "# Plot also the training points\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n",
    "\n",
    "ax.set_title('Perceptron')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More resources\n",
    "\n",
    "* [Scikit-learn implementation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html)\n",
    "* Scikit-learn's [SGDClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) implements a perceptron when the \"perceptron loss\" option is used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In spirit, support vector machines are not that different than perceptrons. They are also a linear classifier, trained with a different loss functions. Since their introduction by Vapnik in 1963, SVMs have gone through multiple incarnations of research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/svm.png)\n",
    "\n",
    "Which of these hyperplanes is the best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuition of the **hard svm**:\n",
    "* Support vector machines try to find a linear separator (hyperplane) that maximizes the distnace between two classes\n",
    "* The points nearest to the hyperplane are known as **support vectors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem formulation is unideal! You usually won't get nice separations between two classes. This is where the **soft SVM** comes in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/soft.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**: Find the maximum margin hyperplane such that if there are violations, penalize the score.\n",
    "\n",
    "In other words, we can write\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min_{\\vec{w}, \\xi} \\qquad& \\frac{1}{2}\\| w \\|^2 + C\\sum_i \\xi_i \\\\\n",
    "s.t.      \\forall i,& y_i (\\vec{w}\\cdot \\vec{x_i}) \\geq 1 - \\xi_i \\\\\n",
    "          \\forall i, &\\xi_i \\geq 0\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In English,\n",
    "\n",
    "* The $\\frac{1}{2} \\| w \\|^2$ represents the margin. There is math that shows that this maximizes the margin (proof needed)\n",
    "* $\\xi$ is the **slack** variable. A penalty is incurred on each $\\vec{x_i}$\n",
    "* The $C$ is a **hyperparameter** (set before training) that determines how much penalty to incur from a sample crossing the margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using algebra to solve for $\\xi_i$, you can show that minimizing this objective minizes the following objective:\n",
    "\n",
    "$$\\min_\\vec{w} \\Bigg( \\frac{1}{2} \\| \\vec{w} \\|^2 + C\\sum_i \\max(0, 1 - y_i (\\vec{w} \\cdot x_i)) \\Bigg)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, you can use SGD to minimize this loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernels\n",
    "\n",
    "Once in a while, you might hear someone mention kernel SVMs. Without getting too deep into it, kernels allow a linear separator to take samples from an original space that is not linearly separable, and embed them into a space where the samples are linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features. We could\n",
    "                      # avoid this ugly slicing by using a two-dim dataset\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = .02  # step size in the mesh\n",
    "\n",
    "# we create an instance of SVM and fit out data. We do not scale our\n",
    "# data since we want to plot the support vectors\n",
    "C = 1.0  # SVM regularization parameter\n",
    "svc = svm.SVC(kernel='linear', C=C).fit(X, y)\n",
    "rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)\n",
    "poly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)\n",
    "lin_svc = svm.LinearSVC(C=C).fit(X, y)\n",
    "\n",
    "# create a mesh to plot in\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# title for the plots\n",
    "titles = ['SVC with linear kernel',\n",
    "          'LinearSVC (linear kernel)',\n",
    "          'SVC with RBF kernel',\n",
    "          'SVC with polynomial (degree 3) kernel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Way to change sizes of plots\n",
    "# http://stackoverflow.com/a/332311/2014591\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "num_zeros = len(y[y == 0])\n",
    "num_ones = len(y[y == 1])\n",
    "num_twos = len(y[y == 2])\n",
    "\n",
    "colors= \"\".join([\"b\"] * num_zeros + [\"r\"] * num_ones + [\"y\"] * num_twos)\n",
    "\n",
    "for i, clf in enumerate((svc, lin_svc, rbf_svc, poly_svc)):\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=colors, cmap=plt.cm.Paired)\n",
    "    plt.xlabel('Sepal length')\n",
    "    plt.ylabel('Sepal width')\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(titles[i])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
