{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Your Models\n",
    "\n",
    "In this part of the tutorial, we'll talk about how to evaluate models\n",
    "* Training procedures - the good, the bad, and the ugly\n",
    "    * Why *not* simple train-test splits\n",
    "    * $k$-fold cross validation\n",
    "* Model metrics\n",
    "    * Confusion matrices, and accuracy scores\n",
    "    * AUC Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Training procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Last time on training procedures\n",
    "\n",
    "![](../applied_ml_1/images/train_test_split.svg)\n",
    "\n",
    "Is this actually a good training procedure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/mnist.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: For your results to be good, your experimental procedures have to be good\n",
    "\n",
    "* Not all 100%'s are created equal. \n",
    "* We need proper procedures of actually evaluating a model\n",
    "\n",
    "## Simple train-test split procedures are erroneous\n",
    "* Depending on your training / testing set split, your accuracy results are going to be different.\n",
    "    * If your algorithm scores well on a testing set, is it *actually* good?\n",
    "    * This is called **model variance**. It relates to how models fit to their training set. (Fitting versus underfitting.)\n",
    "* Goal of machine learning is to create **generalizable algorithms**. We need more robust training methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The more robust way: $k$-fold cross validation\n",
    "\n",
    "1. Split the dataset into $k$ evenly sized sets, called *folds*.\n",
    "2. For each $i$th fold\n",
    "    1. Train the model on each other $j\\neq i$ fold.\n",
    "    2. Evaluate the model on this $i$th fold, record the accuracy\n",
    "3. Record the *average* accuracy over all $k$ folds\n",
    "\n",
    "![](images/cross_validation.svg)\n",
    "\n",
    "\n",
    "Unlike the techniques in my previous tutorial, $k$-fold cross validation isn't a way to improve model classification score. It's a way to publish more legitimate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "prefix = \"../datasets/\"\n",
    "\n",
    "columns=[\"mean corpuscular volume\", \"protein1\", \"protein2\", \n",
    "         \"protein3\", \"protein4\", \"number of drinks\", \"Has Liver Disorder\"]\n",
    "df = pd.read_csv(prefix + \"liver.data\", header=None, names=columns)\n",
    "\n",
    "y = df[\"Has Liver Disorder\"]\n",
    "X = df.drop(\"Has Liver Disorder\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing $k$-fold cross validation in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Documentation: \n",
    "#    http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "total_accuracy = 0\n",
    "kfold = KFold(len(X), n_folds=10, shuffle=True, random_state=0)\n",
    "for train_indices, test_indices in kfold:\n",
    "    X_train, y_train = X.loc[train_indices], y[train_indices]\n",
    "    X_test, y_test = X.loc[test_indices], y[test_indices]\n",
    "    \n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    _X_train = scaler.transform(X_train)\n",
    "    _X_test = scaler.transform(X_test)\n",
    "    \n",
    "    clf = LogisticRegression().fit(_X_train, y_train)\n",
    "    predictions = clf.predict(_X_test)\n",
    "    \n",
    "    total_accuracy += accuracy_score(y_test, predictions)\n",
    "print \"Logistic Regression score after 10-fold cross validation: \", total_accuracy / float(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you wanted an even distribution of classes per fold? Use a **stratified** $k$-fold cross validation procedure. (Below.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More resources\n",
    "* [Machine Learning Mastery](http://machinelearningmastery.com/how-to-choose-the-right-test-options-when-evaluating-machine-learning-algorithms/), discusses other techniques to evaluate a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 2: Model Metrics\n",
    "\n",
    "Why metrics?\n",
    "\n",
    "![](images/yoda.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Model metric**: Describes \"performance\" of your model\n",
    "    * Performing well in one metric does not guarantee success in another\n",
    "    * Your algorithm can be tuned to optimize in one metric instead of another. See [Weston, J., et al.](http://www.thespermwhale.com/jaseweston/papers/wsabie-ijcai.pdf) for an example\n",
    "* Some metrics for classification\n",
    "    * Accuracy scores (from confusion matrices)\n",
    "    * AUC Score\n",
    "* Some metrics I won't get to\n",
    "    * Precision / recall\n",
    "    * F1-scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.1: Accuracy score\n",
    "\n",
    "This is the simpliest of all metrics for classification. Suppose you have a multi-class classification problem. The accuracy score is simply\n",
    "\n",
    "$$\\frac{\\text{Number of samples labeled correctly}}{\\text{Total number of samples}}$$\n",
    "\n",
    "A related data structure is the confusion matrix, which summarizes samples predicted correctly and incorrectly.\n",
    "\n",
    "### Binary classification confusion matrix\n",
    "\n",
    "![](images/confusionmatrix.png)\n",
    "\n",
    "### Multi-class classification confusion matrix \n",
    "\n",
    "* Usually represented by counts, not percentages. Oh well.\n",
    "\n",
    "![](images/confusion2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The [MNIST dataset ](http://yann.lecun.com/exdb/mnist/)\n",
    "\n",
    "* By [Yann LeCun](https://en.wikipedia.org/wiki/Yann_LeCun), currently a research scientist at Facebook\n",
    "* Popular dataset for handwritten digit recognition\n",
    "* Motivated for neural networks because of the high dimensionality. But can use SVM or whatever just as well\n",
    "* Scikit-Learn has utilities that automatically download the digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Source: http://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html\n",
    "\n",
    "# Author: Gael Varoquaux <gael dot varoquaux at normalesup dot org>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "# Standard scientific Python imports\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import datasets, classifiers and performance metrics\n",
    "from sklearn import datasets, svm, metrics\n",
    "\n",
    "# The digits dataset\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The data that we are interested in is made of 8x8 images of digits, let's\n",
    "# have a look at the first 3 images, stored in the `images` attribute of the\n",
    "# dataset.  If we were working from image files, we could load them using\n",
    "# pylab.imread.  Note that each image must have the same size. For these\n",
    "# images, we know which digit they represent: it is given in the 'target' of\n",
    "# the dataset.\n",
    "images_and_labels = list(zip(digits.images, digits.target))\n",
    "for index, (image, label) in enumerate(images_and_labels[:4]):\n",
    "    plt.subplot(2, 4, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Training: %i' % label)\n",
    "\n",
    "# To apply a classifier on this data, we need to flatten the image, to\n",
    "# turn the data in a (samples, feature) matrix:\n",
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1))\n",
    "\n",
    "# Create a classifier: a support vector classifier\n",
    "classifier = svm.SVC(gamma=0.0001)\n",
    "\n",
    "# We learn the digits on the first half of the digits\n",
    "classifier.fit(data[:n_samples / 2], digits.target[:n_samples / 2])\n",
    "\n",
    "# Now predict the value of the digit on the second half:\n",
    "expected = digits.target[n_samples / 2:]\n",
    "predicted = classifier.predict(data[n_samples / 2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (classifier, metrics.classification_report(expected, predicted)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\n",
    "\n",
    "images_and_predictions = list(zip(digits.images[n_samples / 2:], predicted))\n",
    "for index, (image, prediction) in enumerate(images_and_predictions[:4]):\n",
    "    plt.subplot(2, 4, index + 5)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Prediction: %i' % prediction)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2: The AUC Score\n",
    "\n",
    "Up until now, we have assumed that binary classification algorithms can only output binary labels. This is not always the case; we can output probabilities that represent class association.\n",
    "\n",
    "* Let's assume a binary classification setting, and assume class labels $y_i \\in \\{0, 1\\}$\n",
    "* Our previous assumption was $f(\\vec{x}) \\in \\{0, 1\\}$ (or $\\{1, -1\\}$). Now, assume what if $f(\\vec{x}) \\in [0, 1]$, like a probability\n",
    "\n",
    "**Receiver operating characteristic**. By changing the threshold for which we consider $\\vec{x}$ to be of class $0$ or $1$, we collect a *true positive rate* and a *false positive rate*. This is called the receiver operator characteristic curve. (ROC curve).\n",
    "\n",
    "The area underneath the ROC curve is the *AUC* (area under curve). It's between 0 and 1, but don't treat it like a probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The iris dataset\n",
    "\n",
    "* Probably one of the world's most known datasets\n",
    "* Classify the identity of a flower based upon features of the flower's petal size, stem size, etc\n",
    "* Note that SVMs can normally get 100% on this dataset; the procedure below added noise to the feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Source: http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "# Way to change sizes of plots\n",
    "# http://stackoverflow.com/a/332311/2014591\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 10\n",
    "\n",
    "###############################################################################\n",
    "# Data IO and generation\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X, y = X[y != 2], y[y != 2]  # Make this into a binary classification problem instead of multi-class\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "# Add noisy features\n",
    "random_state = np.random.RandomState(0)\n",
    "X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Classification and ROC analysis\n",
    "\n",
    "# Run classifier with cross-validation and plot ROC curves\n",
    "cv = StratifiedKFold(y, n_folds=6)\n",
    "classifier = svm.SVC(kernel='linear', probability=True,\n",
    "                     random_state=random_state)\n",
    "\n",
    "for i, (train, test) in enumerate(cv):\n",
    "    probas_ = classifier.fit(X[train], y[train]).predict_proba(X[test])\n",
    "    fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=1, label='ROC fold %d (area = %0.2f)' % (i, roc_auc))\n",
    "    break # For only one curve\n",
    "\n",
    "plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Luck')\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
