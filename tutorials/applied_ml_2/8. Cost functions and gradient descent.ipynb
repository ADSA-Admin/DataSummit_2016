{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost functions and gradient descent\n",
    "\n",
    "In this section, we're going to talk about specific classification models, and how they're trained (AKA: learned). \n",
    "\n",
    "Concepts\n",
    "* The concept of a cost (loss) function\n",
    "* Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X \\in \\mathbb{R}^{m \\times n}$ be the feature matrix of $m$ samples and $n$ features.\n",
    "For each $\\vec{x}_i$, we associate a corresponding label $y_i \\in \\{-1, 1 \\}$.*\n",
    "\n",
    "Our objective is to <b>learn a function</b> $f : X \\to \\{-1, 1 \\}$.\n",
    "\n",
    "<sup>I'm jumping between $\\{-1, 1\\}$ and $\\{0, 1\\}$. But they're equivalent if you just remap the numbers.</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f$ is learned by learning the **parameters** of the model. Different models have different number of parameters. The learning algorithms below address how to learn the parameters of our model\n",
    "\n",
    "<b>Rule of thumb</b>: The most parameters your model has, the greater number of expressivity it has. ([VC dimension](https://en.wikipedia.org/wiki/VC_dimension))\n",
    "\n",
    "<sup>If you have a probability background, we often say that distributions are *parameterized*. For example, a normal distribution $\\mathcal{N}(\\sigma, \\mu)$ is *parameterized* by $\\sigma$ and $\\mu$. This changes the normal distribution's probability density function and all other related functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost functions\n",
    "\n",
    "Cost functions give a notion of \"loss\" (or error) when $f$ does not correctly label an example. To name a few:\n",
    "\n",
    "* $c(y, \\vec{x}) = \\max(0, -y f(\\vec{x}))$ (Perceptron Loss, Rosenblatt 1957)\n",
    "* $c(y, \\vec{x}) = \\max(0, 1 - yf(\\vec{x}))$ (The Hinge loss)\n",
    "* $c(y, \\vec{x}) = y \\log(f(\\vec{x})) + (1 - y) \\log(1 - f(\\vec{x}))$  (The cross entropy loss)\n",
    "\n",
    "We could define the *average loss* as \n",
    "$$C = \\frac{1}{N}\\sum_i c(y_i, f(\\vec{x}_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overarching paradigm in machine learning:\n",
    "\n",
    "<center><big><b>Everything is an optimization problem </b></big></center>\n",
    "\n",
    "<sup>I can't think of a time when something in machine learning was *not* an optimization problem. If you can. please let me know.</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So given some sort of average loss function $C$ and a function $f$, what we would like\n",
    "to do is minimize $C$ by changing $f$. \n",
    "\n",
    "Without loss of generality, let's assume $f$ is parameterized by \n",
    "* a *weight vector* $\\vec{w} \\in \\mathbb{R}^{d}$\n",
    "* a *bias* $b \\in \\mathbb{R}$. \n",
    "\n",
    "(To remind ourselves of this fact, we'll write $f \\equiv f_{\\vec{w}, b}$.) \n",
    "\n",
    "You could imagine $f$ to be some black box with some dials on the outside for $\\vec{w}$ and\n",
    "$b$; tuning these dials changes the black box function $f$.\n",
    "\n",
    "## Our objective:\n",
    "\n",
    "*Find some weight vector $\\vec{w}$ and a bias $b$ such that the average cost function $C$ is minimized.* By tuning $\\vec{w}$ and $b$, we'll change the value of our average cost,\n",
    "\n",
    "$$C = \\frac{1}{N}\\sum_i c(y_i, f_{\\vec{w}, b}(\\vec{x}_i)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our means: **gradient descent**. \n",
    "\n",
    "### Intuition:\n",
    "\n",
    "At every position in time $t$ determined by $(\\vec{w}^{(t)}, b^{(t)})$, we have some value of the average cost $C$. Step towards $(\\vec{w}^{(t+1)}, b^{(t+1)})$, which is in the direction of *less cost*.\n",
    "\n",
    "![](images/descent.png)\n",
    "\n",
    "*Goal of gradient descent*: Always make a step towards $(\\vec{w}^{(t+1)}, b^{(t+1)})$ until the cost doesn't change. (When our gradient descent **converges***)\n",
    "\n",
    "<sup>There's a discussion of what happens when the algorithm doesn't seem to converge. We'll skip it.</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What direction is $(\\vec{w}^{(t+1)}, b^{(t+1)})$?\n",
    "\n",
    "$$\\text{Simple: Go down the direction of} -\\nabla C$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/gradient-descent-algo.png)\n",
    "\n",
    "Disclaimer: I actually don't know whether $\\nabla_{\\vec{w}, b}$ is common convention.\n",
    "\n",
    "* $\\nabla_{\\vec{w}, b}f = \\Big(\\frac{\\partial f}{\\partial w_1}, \\dots, \\frac{\\partial f}{\\partial w_d}, \\frac{\\partial f}{\\partial b} \\Big)$.  This is a vector of size $n + 1$, and it helps update both the terms in $\\vec{w}$ and $b$.\n",
    "\n",
    "* The **stopping criterion** can be a variety of things\n",
    "    * When $\\vec{w}$ and $b$ don't change significantly after a few iterations\n",
    "    * After a fixed number of iterations\n",
    "    * When the cost is less than some constant (not recommended, but I've seen it done before)\n",
    "\n",
    "* $\\eta^{(t)}$ is called the **learning rate**. In most formulations, $\\eta^{(t)}$ decreases after a fixed number of steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Stochastic gradient descent\n",
    "\n",
    "* For gradient descent, have to compute a cost per sample in the dataset before making a step\n",
    "* Might be faster to step just after every sample:\n",
    "    $$\\nabla c(y, f(\\vec{x}))\\big\\vert_{\\vec{x} = \\vec{x_i}, y = y_i}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of gradient descent\n",
    "\n",
    "Gradient descent isn't a learning algorithm; it's an optimization method. However, we'll discuss how it is the **basis** for many learning algorithms, such as\n",
    "\n",
    "* Perceptrons\n",
    "* Logistic regressions\n",
    "* Support vector machines\n",
    "* Neural networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
