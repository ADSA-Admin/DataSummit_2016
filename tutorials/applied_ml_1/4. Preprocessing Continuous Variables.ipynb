{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Continuous Variables\n",
    "\n",
    "This tutorial will present various methods on how to preprocess continuous variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap on UCI Breast Cancer Dataset (breast.data)\n",
    "\n",
    "* Easy dataset to start off with\n",
    "* Dataset contains all continuous variables, except one ID column, and one label (M, B) column\n",
    "    * The continous variables are just statistics collected from a tumor's biopsy\n",
    "    * More information can be found [here](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.names)\n",
    "* Goal of the dataset is to classify whether a tumor is maligant (M) or benigh (B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prefix = \"../datasets/\"\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(prefix + \"breast.data\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.drop(0, axis=1, inplace=True)\n",
    "\n",
    "# Creating features and response variable set\n",
    "y = df[1]\n",
    "X = df.drop(1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = SVC().fit(X_train, y_train).predict(X_test)\n",
    "non_scale_accuracy = accuracy_score(y_test, predictions)\n",
    "print \"Accuracy of SVM: \", non_scale_accuracy\n",
    "\n",
    "predictions = LogisticRegression().fit(X_train, y_train).predict(X_test)\n",
    "non_scale_accuracy = accuracy_score(y_test, predictions)\n",
    "print \"Accuracy of Logistic Regression: \", non_scale_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the classification rate by feature engineering\n",
    "\n",
    "In general, we're not getting the bang for our buck using the support vector machine. And it's because we're not preprocessing the continuous features correctly.\n",
    "\n",
    "A variety of ways to improve model accuracy with continuous features\n",
    "\n",
    "* Feature scaling\n",
    "    * Standard scaling: For each continuous feature, $\\mu = 0$ and $\\sigma = 1$\n",
    "    * Min-max scaling: Scale all continuous features between the range $[0, 1]$ or $[-1, 1]$.\n",
    "* Univariate feature selection\n",
    "    * Using the Chi-squared statistic to improve classification\n",
    "* Discretization of continuous features\n",
    "    * No real example, just a survey of techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Feature scaling\n",
    "\n",
    "* Idea is that continuous features can take anywhere in a certain range; need a way to shrink (or inflate) everything\n",
    "* Reduce the variation in the dataset using scaling.\n",
    "* **Standard scaling** applies the following formula to transform a feature into a space with mean 0 and standard deviation 1. This is also called \"recentering\" the dataset.\n",
    "\n",
    "    Given the $i$th continuous feature $X_i$, we apply the following formula for each $x \\in X_i$:\n",
    "    $$x' = \\frac{x - \\bar{X_i}}{\\sigma_{X_i}}$$\n",
    "    where $\\bar{X_i}$ is the mean of feature $X_i$ and $\\sigma_{X_i}$ is its standard deviation. Our new dataset composed of $x'$ will have mean 0 and standard deviation 1.\n",
    "* **Min-max scaling** applies the following formula to shrink (or inflate) features into a space between a given interval. If we want our features to lie within the interval [0, 1], the following formula would work.\n",
    "    $$x' = \\frac{x - \\min(X_i)}{\\max(X_i) - \\min(X_i)}$$\n",
    "\n",
    "* More information on [Wikipedia](https://en.wikipedia.org/wiki/Feature_scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# I want to show that SVMs are sensitive to feature scaling.\n",
    "# In partcular, because sklearn.svm.SVC uses the RBF kernel, this kernel\n",
    "# is sensitive to scaling.\n",
    "#\n",
    "# More information on how to properly train an SVM is here:\n",
    "#    http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf\n",
    "\n",
    "for Scaler in [StandardScaler, MinMaxScaler]:\n",
    "    \n",
    "    # \"Scaler\" is a class object whose constructor and attributes we can call\n",
    "    scaler = Scaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    \n",
    "    svm = SVC().fit(X_train_scaled, y_train)\n",
    "    \n",
    "    X_test_scaled = scaler.transform(X_test)  # Note we don't \"refit\" for testing data\n",
    "    predictions = svm.predict(X_test_scaled)\n",
    "    print \"Accuracy of SVM using {0}: {1}\".format(Scaler.__name__, accuracy_score(y_test, predictions))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Univariate Feature Selection\n",
    "    \n",
    "Idea is that we have all of these continuous attributes.... who is to say that any of them are useful?\n",
    "\n",
    "The full Scikit-Learn module on feature selection is presented [here](http://scikit-learn.org/stable/modules/feature_selection.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## UCI Sonar Dataset\n",
    "\n",
    "* The task is to train a classifier to discriminate between sonar signals bounced off a metal cylinder and those bounced off a roughly cylindrical rock. (From website.)\n",
    "* More dataset description [here](https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.names). The dataset was originally used for neural network classification tasks.\n",
    "* In general, this is one of my favorite datasets because the classification task is difficult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(prefix + \"sonar.data\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = df.drop(60, axis=1)\n",
    "y = df[60]  # Rock or mine class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# As a baseline, let's classify this with Logistic Regression, no scaling\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = LogisticRegression().fit(X_train, y_train).predict(X_test)\n",
    "baseline_acc = accuracy_score(predictions, y_test)\n",
    "print \"Accuracy of Logistic Regression: \", baseline_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate feature selection definition\n",
    "\n",
    "* Not all features in your training set help predict the output\n",
    "* **Univariate feature selection** is a technique to weed out features that are uninformative.\n",
    "* Using a *scoring function*, we rank variables by their scores, and discard variables that yield a low score\n",
    "    * For classification: [chi-squared](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2) or [ANOVA F-value](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif)\n",
    "    * In Scikit-Learn, can either select variables using percentiles ([SelectPercentile](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html#sklearn.feature_selection.SelectPercentile))  or choose a fixed $k$ variables ([SelectKBest](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html))\n",
    "    \n",
    "References\n",
    "* [Scikit-Learn user guide](http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection) (describes what univariate feature selection is, and how to use it)\n",
    "* [Scikit-Learn example using SVMs and iris dataset](http://scikit-learn.org/stable/auto_examples/feature_selection/plot_feature_selection.html)\n",
    "* [Another Scikit-Learn example using SVMs](http://scikit-learn.org/stable/auto_examples/svm/plot_svm_anova.html)\n",
    "* [Wikipedia's example about the Chi-squared statistic](https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test#Examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical procedure using the chi-squared statistic\n",
    "\n",
    "Let \n",
    "* $C_j$ be the random variable representing the *class label* of sample $j$. The observed value of $C_j$ is also called the **response**.\n",
    "* $X$ be the random variable representing the value of some feature in the dataset. We assume that $X$ is normally distributed.\n",
    "\n",
    "Our null hypothesis is that $X$ and $C_j$ are independent, that is:\n",
    "\n",
    "$$P(C_j \\,\\vert\\, X) = P(C_j).$$\n",
    "\n",
    "In other words, there is *no* relationship between our feature and the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chi-squared statistic helps us assign a - the **$\\chi^2$ statistic** - to each of our features. Depending on the threshold we set, we will either accept or reject our null hypothesis. \n",
    "* If we *accept* our null hypothesis, then the response is independent of the feature, and we can throw the feature away. \n",
    "* If we *reject* our null hypothesis, then the response is dependent of the feature, and we should keep this feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "* [Chi-squared feature selection in document classification](http://nlp.stanford.edu/IR-book/html/htmledition/feature-selectionchi2-feature-selection-1.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "statistics, pvalues = chi2(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = sorted(enumerate(statistics), key=lambda (index, value): value)\n",
    "for (index, value) in features[:20]:\n",
    "    print \"Feature {0} with chi-squared {1}\".format(index, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example using SelectKBest and the Chi-squared statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "n_features = X.shape[1]\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "accuracies = list()\n",
    "\n",
    "selector = SelectKBest(score_func=chi2)\n",
    "for k_chosen in xrange(1, n_features, 1):\n",
    "    selector.set_params(k=k_chosen)\n",
    "    \n",
    "    X_altered = selector.fit_transform(X, y)\n",
    "    \n",
    "    predictions = LogisticRegression().fit(X_altered, y).predict(X_altered)\n",
    "    \n",
    "    accuracy = accuracy_score(predictions, y)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "features = range(1, n_features, 1)\n",
    "plt.plot(features, accuracies, label=\"Logistic Regression\")\n",
    "plt.title(\"Performance of Logistic Regression using univariate feature selection\\nwith respect to ANOVA F-score\")\n",
    "plt.xlabel(\"Accuracy Score\")\n",
    "plt.ylabel(\"Number of features selected\")\n",
    "plt.ylim(0.65, 1)\n",
    "\n",
    "# Plot the baseline\n",
    "plt.plot(features, [baseline_acc] * len(features), label=\"Baseline LR\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "index, acc = max(enumerate(accuracies), key=lambda (i, acc): acc)\n",
    "print \"Best accuracy is when using {0} features\".format(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warning: Don't overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_features = X_train.shape[1]\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "accuracies2 = list()\n",
    "\n",
    "selector = SelectKBest(score_func=chi2)\n",
    "for k_chosen in xrange(1, n_features, 1):\n",
    "    selector.set_params(k=k_chosen)\n",
    "    \n",
    "    X_train_altered = selector.fit_transform(X_train, y_train)\n",
    "    X_test_altered = selector.transform(X_test)\n",
    "    \n",
    "    predictions = LogisticRegression().fit(X_train_altered, y_train).predict(X_test_altered)\n",
    "    \n",
    "    accuracy = accuracy_score(predictions, y_test)\n",
    "    accuracies2.append(accuracy)\n",
    "\n",
    "plt.title(\"Performance of Logistic Regression using univariate feature selection\\nwith respect to ANOVA F-score\")\n",
    "plt.xlabel(\"Accuracy Score\")\n",
    "plt.ylabel(\"Number of features selected\")\n",
    "plt.ylim(0.65, 1)\n",
    "    \n",
    "    \n",
    "features = range(1, n_features, 1)\n",
    "plt.plot(features, accuracies, label=\"Overfit LR\")\n",
    "plt.plot(features, accuracies2, label=\"Not-overfit LR\")\n",
    "\n",
    "# Plot the baseline\n",
    "plt.plot(features, [baseline_acc] * len(features), label=\"Baseline LR\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#index, acc = max(enumerate(accuracies), key=lambda (i, acc): acc)\n",
    "#print \"Best accuracy is when using {0} features\".format(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More resources on feature selection\n",
    "\n",
    "* [Quora answer by Olivier Grisel](https://www.quora.com/How-do-I-perform-feature-selection)\n",
    "* \n",
    "[Isabelle Guyon and Andr√© Elisseeff. 2003. An introduction to variable and feature selection. J. Mach. Learn. Res. 3 (March 2003), 1157-1182.](http://www.jmlr.org/papers/volume3/guyon03a/guyon03a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Discretization of continuous features\n",
    "\n",
    "* A large value for a continuous feature could mean a tendency to be of class $A$ instead of class $B$.\n",
    "* Doesn't take into account that continuous features could be \"clustered\"\n",
    "    * *Example*: Suppose a continuous feature $x$ for some sample $i$ took values between $[0, 1]$. We want to classify $i$ to be either class $A$, $B$, or $C$\n",
    "        * $x \\in [0, 0.33)$ implies $i$ tends to be in class $A$\n",
    "        * $x \\in [0.33, 0.66)$ implies $i$ tends to be in class $B$\n",
    "        * $x \\in [0.66, 1]$ implies $i$ tends to be in class $C$\n",
    "* A continuous (numeric) feature doesn't help distinguish the three classes. But there are several **discretization** (ie *binning*) methods which convert the continuous feature into a categorical one\n",
    "\n",
    "Historically, this was a fun research problem back when. For decision trees, it was non-trivial to figure out where to split a continuous interval. The [C4.5](https://en.wikipedia.org/wiki/C4.5_algorithm#Improvements_from_ID.3_algorithm) algorithm improved Ross Quinlan's [ID3](https://en.wikipedia.org/wiki/ID3_algorithm) algorithm.\n",
    "\n",
    "With all of this said, I don't think discretization is a popular machine learning problem anymore. (Citation needed.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glossery of techniques\n",
    "* Unsupervised binning\n",
    "    * **Simple uniform binning**: Split an interval into $k$ evenly sized intervals.\n",
    "    * **Percentile / quartile based binning**: Split an interval based on percentiles or quartiles\n",
    "    * **Random forest embedding**: Implementation [here](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomTreesEmbedding.html#sklearn.ensemble.RandomTreesEmbedding)\n",
    "* Supervised binning\n",
    "    * **Minimum description length principal.** Implementation [here](https://github.com/UIUC-data-mining/mdlp-discretization), paper [here](http://ijcai.org/Past%20Proceedings/IJCAI-93-VOL2/PDF/022.pdf)\n",
    "\n",
    "A survey of discretization methods applied to decision tree learning is provided [here](http://download.springer.com/static/pdf/961/art%253A10.1023%252FA%253A1016304305535.pdf?originUrl=http%3A%2F%2Flink.springer.com%2Farticle%2F10.1023%2FA%3A1016304305535&token2=exp=1454459259~acl=%2Fstatic%2Fpdf%2F961%2Fart%25253A10.1023%25252FA%25253A1016304305535.pdf%3ForiginUrl%3Dhttp%253A%252F%252Flink.springer.com%252Farticle%252F10.1023%252FA%253A1016304305535*~hmac=daa83bbb881f0e61183afb0904117e3e194d3ce0dce7aaf518f624483a146500)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/discretization.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
